defaults:
  - model: resnet50
  - trainer: default

paths:
  output_dir: ${hydra:runtime.output_dir}

data_module:
  _target_: src.basic_lvl3.CIFAR10DataModule
  data_dir: data/
  batch_size: 64
  num_workers: 8
  # transforms for imagenet
  train_transform:
    _target_: torchvision.transforms.Compose
    transforms:
      - _target_: torchvision.transforms.RandomHorizontalFlip
      - _target_: torchvision.transforms.RandomAffine
        degrees: 15
        translate: [0.1, 0.1]
        scale: [0.9, 1.1]
      - _target_: torchvision.transforms.ColorJitter
        brightness: 0.1
        contrast: 0.1
        saturation: 0.1
        hue: 0.1
      - _target_: torchvision.transforms.RandomResizedCrop
        size: 32
        scale: [0.9, 1.1]
        ratio: [0.9, 1.1]
      - _target_: torchvision.transforms.ToTensor
      - _target_: torchvision.transforms.Normalize
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

  test_transform:
    _target_: torchvision.transforms.Compose
    transforms:
      - _target_: torchvision.transforms.ToTensor
      - _target_: torchvision.transforms.Normalize
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

logger:
  _target_: lightning.pytorch.loggers.wandb.WandbLogger
  # name: "" # name of the run (normally generated by wandb)
  save_dir: "${paths.output_dir}"
  offline: False
  id: null # pass correct id to resume experiment!
  anonymous: null # enable anonymous logging
  project: "basic-lvl3"
  log_model: False # upload lightning ckpts
  prefix: "" # a string to put at the beginning of metric keys
  # entity: "" # set to name of your wandb team
  group: ""
  tags: []
  job_type: ""

callbacks:
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: ${paths.output_dir}/checkpoints # directory to save the model file
    filename: "best_epoch_{epoch:03d}" # checkpoint filename
    monitor: "val/acc" # name of the logged metric which determines when model is improving
    verbose: False # verbosity mode
    save_last: True # additionally always save an exact copy of the last checkpoint to a file last.ckpt
    save_top_k: 1 # save k best models (determined by above metric)
    mode: "max" # "max" means higher metric value is better, can be also "min"
    auto_insert_metric_name: False # when True, the checkpoints filenames will contain the metric name
    save_weights_only: False # if True, then only the modelâ€™s weights will be saved
    every_n_train_steps: null # number of training steps between checkpoints
    train_time_interval: null # checkpoints are monitored at the specified time interval
    every_n_epochs: null # number of epochs between checkpoints
    save_on_train_epoch_end: null # whether to run checkpointing at the end of the training epoch or the end of validation
  - _target_: lightning.pytorch.callbacks.RichProgressBar
  - _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: "val/acc" # quantity to be monitored, must be specified !!!
    min_delta: 0. # minimum change in the monitored quantity to qualify as an improvement
    patience: 3 # number of checks with no improvement after which training will be stopped
    verbose: False # verbosity mode
    mode: "max" # "max" means higher metric value is better, can be also "min"
    strict: True # whether to crash the training if monitor is not found in the validation metrics
    check_finite: True # when set True, stops training when the monitor becomes NaN or infinite
    stopping_threshold: null # stop training immediately once the monitored quantity reaches this threshold
    divergence_threshold: null # stop training as soon as the monitored quantity becomes worse than this threshold
    check_on_train_epoch_end: null # whether to run early stopping at the end of the training epoch
    # log_rank_zero_only: False  # this keyword argument isn't available in stable version
  # - _target_: lightning.pytorch.callbacks.ModelPruning
  #   pruning_fn: l1_unstructured
  #   amount: 0.5
  # fast_dev_run: true
  # limit_train_batches: 100
  # limit_val_batches: 100
  # limit_test_batches: 100
